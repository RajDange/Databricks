{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff80633-9d84-4814-bf47-212e7cfc9c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (Databricks will run this at the start of the job)\n",
    "# You can use %pip in a notebook cell or subprocess in a script\n",
    "try:\n",
    "    from datasketch import MinHash, MinHashLSH\n",
    "    from Levenshtein import ratio as levenshtein_ratio\n",
    "except ImportError:\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasketch\", \"python-Levenshtein\"])\n",
    "    from datasketch import MinHash, MinHashLSH\n",
    "    from Levenshtein import ratio as levenshtein_ratio\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf, concat_ws, lower, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"DeduplicationJob\").getOrCreate()\n",
    "\n",
    "# Normalization functions\n",
    "def preprocess_text(text):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    \"ch\": \"switzerland\",\n",
    "    \"suisse\": \"switzerland\",\n",
    "    \"schweiz\": \"switzerland\",\n",
    "    \"li\": \"liechtenstein\",\n",
    "    \"us\": \"united states\",\n",
    "    \"gb\": \"united kingdom\",\n",
    "    \"de\": \"germany\"\n",
    "}\n",
    "\n",
    "def preprocess_country_names(country):\n",
    "    country = preprocess_text(country)\n",
    "    return COUNTRY_MAP.get(country, country)\n",
    "\n",
    "# UDFs for Spark\n",
    "preprocess_text_udf = udf(lambda x: preprocess_text(x), StringType())\n",
    "preprocess_country_names_udf = udf(lambda x: preprocess_country_names(x), StringType())\n",
    "\n",
    "# Read input table\n",
    "input_table = \"my_database.my_schema.dedup_input\"\n",
    "df_spark = spark.table(input_table)\n",
    "\n",
    "# Normalize column names for pandas\n",
    "columns_map = {\n",
    "    \"FUSION_CUSTOMER_NAME\": \"fusion_customer_name\",\n",
    "    \"ADDRESS_LINE_1\": \"address_line_1\",\n",
    "    \"POSTAL_CODE\": \"postal_code\",\n",
    "    \"CITY\": \"city\",\n",
    "    \"COUNTRY\": \"country\",\n",
    "    \"ID\": \"id\",\n",
    "    \"SOURCE_SYSTEM\": \"source_system\"\n",
    "}\n",
    "\n",
    "# Select and rename columns\n",
    "df_spark = df_spark.select(*columns_map.keys())\n",
    "for orig, new in columns_map.items():\n",
    "    df_spark = df_spark.withColumnRenamed(orig, new)\n",
    "\n",
    "# Create 'full_address' column (normalize address_line_1 only, as there's no address_line_2)\n",
    "df_spark = df_spark.withColumn(\"full_address\", preprocess_text_udf(col(\"address_line_1\")))\n",
    "\n",
    "# Normalize other columns\n",
    "df_spark = df_spark.withColumn(\"fusion_customer_name\", preprocess_text_udf(col(\"fusion_customer_name\")))\n",
    "df_spark = df_spark.withColumn(\"city\", preprocess_text_udf(col(\"city\")))\n",
    "df_spark = df_spark.withColumn(\"country\", preprocess_country_names_udf(col(\"country\")))\n",
    "\n",
    "# Convert Spark DataFrame -> Pandas DataFrame for MinHash deduplication\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "# Deduplication algorithm (unchanged except column names)\n",
    "def create_weighted_minhash(row, columns, weights):\n",
    "    combined_text = []\n",
    "    for col, weight in zip(columns, weights):\n",
    "        if col in row:\n",
    "            normalized_value = str(row[col])\n",
    "            combined_text.extend([normalized_value] * weight)  # Repeat based on weight\n",
    "    combined_text = \" \".join(combined_text)\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    for word in combined_text.split():\n",
    "        minhash.update(word.encode(\"utf8\"))\n",
    "    return minhash\n",
    "\n",
    "def weighted_clustering(df, columns, weights, threshold):\n",
    "    lsh = MinHashLSH(threshold=threshold / 100, num_perm=128)\n",
    "    clusters = defaultdict(list)\n",
    "    minhashes = {}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        minhash = create_weighted_minhash(row, columns, weights)\n",
    "        minhashes[idx] = minhash\n",
    "        lsh.insert(idx, minhash)\n",
    "    \n",
    "    for idx in minhashes:\n",
    "        cluster = lsh.query(minhashes[idx])\n",
    "        clusters[frozenset(cluster)].append(idx)\n",
    "    \n",
    "    cluster_sizes = {}\n",
    "    cluster_id = 1\n",
    "    cluster_assignments = [None] * len(df)\n",
    "    driver_ids = [None] * len(df)\n",
    "    link_scores = [None] * len(df)\n",
    "    \n",
    "    for cluster in clusters.values():\n",
    "        size = len(cluster)\n",
    "        driver_idx = cluster[0]\n",
    "        driver_row_id = df.iloc[driver_idx]['id']\n",
    "        \n",
    "        for idx in cluster:\n",
    "            row_id = df.iloc[idx]['id']\n",
    "            cluster_assignments[idx] = cluster_id\n",
    "            driver_ids[idx] = driver_row_id\n",
    "            link_scores[idx] = levenshtein_ratio(df.iloc[driver_idx][columns[0]], df.iloc[idx][columns[0]]) * 100\n",
    "            df.at[idx, \"Row_Id\"] = row_id\n",
    "        \n",
    "        cluster_sizes[cluster_id] = size\n",
    "        cluster_id += 1\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        if cluster_assignments[idx] is None:\n",
    "            row_id = df.iloc[idx]['id']\n",
    "            cluster_assignments[idx] = row_id\n",
    "            driver_ids[idx] = row_id\n",
    "            link_scores[idx] = 0.0\n",
    "            df.at[idx, \"Row Id\"] = row_id\n",
    "    \n",
    "    return cluster_assignments, driver_ids, cluster_sizes, link_scores\n",
    "\n",
    "# Hardcoded thresholds/columns/weights\n",
    "thresholds = {\n",
    "    \"combined\": {\n",
    "        \"columns\": [\"fusion_customer_name\", \"full_address\", \"city\", \"country\"],\n",
    "        \"weights\": [5, 5, 2, 1],\n",
    "        \"threshold\": 80\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run clustering\n",
    "combined_columns = thresholds[\"combined\"][\"columns\"]\n",
    "weights = thresholds[\"combined\"][\"weights\"]\n",
    "threshold = thresholds[\"combined\"][\"threshold\"]\n",
    "\n",
    "cluster_ids, driver_ids, cluster_sizes, link_scores = weighted_clustering(df, combined_columns, weights, threshold)\n",
    "\n",
    "# Add results to DataFrame\n",
    "df[\"Cluster Id\"] = cluster_ids\n",
    "df[\"Driver Id\"] = driver_ids\n",
    "df[\"Cluster Size\"] = [cluster_sizes[cluster_id] for cluster_id in cluster_ids]\n",
    "df[\"LinkScore\"] = link_scores\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "df_spark_out = spark.createDataFrame(df)\n",
    "\n",
    "# Write to output table (overwrite mode)\n",
    "output_table = \"my_database.my_schema.dedup_output\"\n",
    "df_spark_out.write.mode(\"overwrite\").saveAsTable(output_table)\n",
    "print(f\"Clustered data saved to {output_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6292943626781771,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Dedup_Test_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
