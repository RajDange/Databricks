{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476a22f0-5984-4d97-bf07-d244c475f5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasketch python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3bde2c3e-ebf0-43a6-8412-dc0c26cfaa8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a050b8a3-f5ae-496d-80e3-91904cf5d0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text).strip().lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    \"ch\": \"switzerland\",\n",
    "    \"suisse\": \"switzerland\",\n",
    "    \"schweiz\": \"switzerland\",\n",
    "    \"li\": \"liechtenstein\",\n",
    "    \"us\": \"united states\",\n",
    "    \"gb\": \"united kingdom\",\n",
    "    \"de\": \"germany\"\n",
    "}\n",
    "\n",
    "def preprocess_country_names(country):\n",
    "    country = preprocess_text(country)\n",
    "    return COUNTRY_MAP.get(country, country)\n",
    "\n",
    "def create_weighted_minhash(row, columns, weights):\n",
    "    combined_text = []\n",
    "    for col, weight in zip(columns, weights):\n",
    "        if col in row:\n",
    "            normalized_value = str(row[col])\n",
    "            combined_text.extend([normalized_value] * weight)\n",
    "    combined_text = \" \".join(combined_text)\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    for word in combined_text.split():\n",
    "        minhash.update(word.encode(\"utf8\"))\n",
    "    return minhash\n",
    "\n",
    "def weighted_clustering(df, columns, weights, threshold):\n",
    "    lsh = MinHashLSH(threshold=threshold / 100, num_perm=128)\n",
    "    clusters = defaultdict(list)\n",
    "    minhashes = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        minhash = create_weighted_minhash(row, columns, weights)\n",
    "        minhashes[idx] = minhash\n",
    "        lsh.insert(idx, minhash)\n",
    "\n",
    "    visited = set()\n",
    "    cluster_assignments = [None] * len(df)\n",
    "    driver_ids = [None] * len(df)\n",
    "    link_scores = [None] * len(df)\n",
    "    cluster_id = 1\n",
    "    cluster_sizes = {}\n",
    "\n",
    "    for idx in minhashes:\n",
    "        if idx in visited:\n",
    "            continue\n",
    "        cluster = lsh.query(minhashes[idx])\n",
    "        # Cluster is a list of indices\n",
    "        for member_idx in cluster:\n",
    "            visited.add(member_idx)\n",
    "            cluster_assignments[member_idx] = cluster_id\n",
    "            driver_ids[member_idx] = df.iloc[idx]['id']\n",
    "            # LinkScore: Levenshtein similarity between main field and driver\n",
    "            link_scores[member_idx] = levenshtein_ratio(\n",
    "                str(df.iloc[idx][columns[0]]),\n",
    "                str(df.iloc[member_idx][columns[0]])\n",
    "            ) * 100\n",
    "        cluster_sizes[cluster_id] = len(cluster)\n",
    "        cluster_id += 1\n",
    "\n",
    "    # Any records not assigned? (shouldn't happen, but just in case)\n",
    "    for idx in range(len(df)):\n",
    "        if cluster_assignments[idx] is None:\n",
    "            cluster_assignments[idx] = cluster_id\n",
    "            driver_ids[idx] = df.iloc[idx]['id']\n",
    "            link_scores[idx] = 100.0\n",
    "            cluster_sizes[cluster_id] = 1\n",
    "            cluster_id += 1\n",
    "\n",
    "    return cluster_assignments, driver_ids, cluster_sizes, link_scores\n",
    "\n",
    "# ------------------- DLT TABLE FUNCTION -------------------\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"dedup_output_Python_DLT\"\n",
    ")\n",
    "def deduplication_fuzzy():\n",
    "    # Replace below with your input table reference\n",
    "    input_table = \"my_database.my_schema.dedup_input\"\n",
    "\n",
    "    # Load the data as Spark DataFrame\n",
    "    df_spark = spark.table(input_table)\n",
    "    # If your data is already clean, skip normalization below\n",
    "    # Rename columns to lower case for pandas\n",
    "    df_spark = df_spark.select(\n",
    "        df_spark[\"FUSION_CUSTOMER_NAME\"].alias(\"fusion_customer_name\"),\n",
    "        df_spark[\"ADDRESS_LINE_1\"].alias(\"address_line_1\"),\n",
    "        df_spark[\"POSTAL_CODE\"].alias(\"postal_code\"),\n",
    "        df_spark[\"CITY\"].alias(\"city\"),\n",
    "        df_spark[\"COUNTRY\"].alias(\"country\"),\n",
    "        df_spark[\"ID\"].alias(\"id\"),\n",
    "        df_spark[\"SOURCE_SYSTEM\"].alias(\"source_system\")\n",
    "    )\n",
    "\n",
    "    # Collect to pandas DataFrame (safe for ~1000 rows)\n",
    "    df = df_spark.toPandas()\n",
    "\n",
    "    # Data normalization (skip if not needed)\n",
    "    df[\"country\"] = df[\"country\"].apply(preprocess_country_names)\n",
    "    df[\"fusion_customer_name\"] = df[\"fusion_customer_name\"].apply(preprocess_text)\n",
    "    df[\"address_line_1\"] = df[\"address_line_1\"].apply(preprocess_text)\n",
    "    df[\"city\"] = df[\"city\"].apply(preprocess_text)\n",
    "    df[\"full_address\"] = df[\"address_line_1\"]\n",
    "\n",
    "    # Clustering (fuzzy deduplication)\n",
    "    combined_columns = [\"fusion_customer_name\", \"full_address\", \"city\", \"country\"]\n",
    "    weights = [5, 5, 2, 1]\n",
    "    threshold = 80\n",
    "\n",
    "    cluster_ids, driver_ids, cluster_sizes, link_scores = weighted_clustering(df, combined_columns, weights, threshold)\n",
    "    df[\"Cluster_Id\"] = cluster_ids\n",
    "    df[\"Driver_Id\"] = driver_ids\n",
    "    df[\"Cluster_Size\"] = [cluster_sizes[cid] for cid in cluster_ids]\n",
    "    df[\"LinkScore\"] = link_scores\n",
    "\n",
    "    # Convert back to Spark DataFrame for DLT output\n",
    "    df_spark_out = spark.createDataFrame(df)\n",
    "    return df_spark_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059bb62f-10df-40c9-ad24-60a41b82ecce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6292943626781771,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Dedup_Test_Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
