{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a050b8a3-f5ae-496d-80e3-91904cf5d0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Native Fuzzy Deduplication using Levenshtein Distance (No pandas, No toPandas)\n",
    "# This solution uses only PySpark built-ins, so it is scalable and distributed.\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dedup_output_PySpark_DLT\"\n",
    ")\n",
    "def deduplication_spark_native():\n",
    "    # Replace with your actual input table path\n",
    "    input_table = \"my_database.my_schema.dedup_input\"\n",
    "\n",
    "    # Load data\n",
    "    df = spark.table(input_table).select(\n",
    "        F.col(\"FUSION_CUSTOMER_NAME\").alias(\"fusion_customer_name\"),\n",
    "        F.col(\"ADDRESS_LINE_1\").alias(\"address_line_1\"),\n",
    "        F.col(\"POSTAL_CODE\").alias(\"postal_code\"),\n",
    "        F.col(\"CITY\").alias(\"city\"),\n",
    "        F.col(\"COUNTRY\").alias(\"country\"),\n",
    "        F.col(\"ID\").alias(\"id\"),\n",
    "        F.col(\"SOURCE_SYSTEM\").alias(\"source_system\")\n",
    "    )\n",
    "\n",
    "    # (Optional) Normalize string columns\n",
    "    df = df.withColumn(\"fusion_customer_name_norm\", \n",
    "                       F.lower(F.regexp_replace(F.col(\"fusion_customer_name\"), r'[^a-z0-9\\s]', '')))\n",
    "    df = df.withColumn(\"address_line_1_norm\", \n",
    "                       F.lower(F.regexp_replace(F.col(\"address_line_1\"), r'[^a-z0-9\\s]', '')))\n",
    "    df = df.withColumn(\"city_norm\", \n",
    "                       F.lower(F.regexp_replace(F.col(\"city\"), r'[^a-z0-9\\s]', '')))\n",
    "    df = df.withColumn(\"country_norm\", \n",
    "                       F.lower(F.regexp_replace(F.col(\"country\"), r'[^a-z0-9\\s]', '')))\n",
    "    # You can skip normalization if input is already clean\n",
    "\n",
    "    # Combine relevant columns into one string for fuzzy matching\n",
    "    df = df.withColumn(\n",
    "        \"combined_key\",\n",
    "        F.concat_ws(\n",
    "            \" \",\n",
    "            F.col(\"fusion_customer_name_norm\"),\n",
    "            F.col(\"address_line_1_norm\"),\n",
    "            F.col(\"city_norm\"),\n",
    "            F.col(\"country_norm\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Generate a unique row number for each record (for join logic)\n",
    "    df = df.withColumn(\"row_num\", F.monotonically_increasing_id())\n",
    "\n",
    "    # Self-join with windowing to limit the comparison scope for scalability\n",
    "    window = Window.orderBy(\"fusion_customer_name_norm\")\n",
    "    df = df.withColumn(\"window_id\", F.row_number().over(window))\n",
    "\n",
    "    # For each record, compare only with nearby records (window of 100, adjust as needed)\n",
    "    window_size = 100\n",
    "    df1 = df.alias(\"df1\")\n",
    "    df2 = df.alias(\"df2\")\n",
    "\n",
    "    join_condition = (\n",
    "        (F.abs(F.col(\"df1.window_id\") - F.col(\"df2.window_id\")) <= window_size) &\n",
    "        (F.col(\"df1.row_num\") < F.col(\"df2.row_num\"))\n",
    "    )\n",
    "\n",
    "    # Compute Levenshtein distance between combined keys\n",
    "    joined = df1.join(df2, join_condition, how=\"inner\") \\\n",
    "        .withColumn(\n",
    "            \"levenshtein_dist\",\n",
    "            F.levenshtein(F.col(\"df1.combined_key\"), F.col(\"df2.combined_key\"))\n",
    "        )\n",
    "\n",
    "    # Set your similarity threshold (lower distance = more similar)\n",
    "    max_distance = 5  # Adjust as needed\n",
    "\n",
    "    similar_pairs = joined.filter(F.col(\"levenshtein_dist\") <= max_distance) \\\n",
    "        .select(\n",
    "            F.col(\"df1.id\").alias(\"id_1\"),\n",
    "            F.col(\"df2.id\").alias(\"id_2\"),\n",
    "            F.col(\"levenshtein_dist\")\n",
    "        )\n",
    "\n",
    "    # For clustering: assign minimum id as the cluster leader (simple approach)\n",
    "    cluster_leader_df = similar_pairs.withColumn(\n",
    "        \"cluster_leader\",\n",
    "        F.least(\"id_1\", \"id_2\")\n",
    "    )\n",
    "\n",
    "    # Create mapping from id to cluster leader\n",
    "    id_to_leader = cluster_leader_df.select(\"id_1\", \"cluster_leader\") \\\n",
    "        .union(cluster_leader_df.select(F.col(\"id_2\").alias(\"id_1\"), \"cluster_leader\")) \\\n",
    "        .distinct()\n",
    "\n",
    "    # Join cluster info back to the original\n",
    "    df_with_cluster = df.join(\n",
    "        id_to_leader,\n",
    "        df.id == id_to_leader.id_1,\n",
    "        how=\"left\"\n",
    "    ).withColumn(\n",
    "        \"Cluster_Id\",\n",
    "        F.coalesce(F.col(\"cluster_leader\"), F.col(\"id\"))  # If not matched, self as cluster leader\n",
    "    ).drop(\"id_1\", \"cluster_leader\")\n",
    "\n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = df_with_cluster.groupBy(\"Cluster_Id\").count().withColumnRenamed(\"count\", \"Cluster_Size\")\n",
    "    df_with_cluster = df_with_cluster.join(cluster_sizes, \"Cluster_Id\", how=\"left\")\n",
    "\n",
    "    # Add Row_Id for compatibility with previous output\n",
    "    df_with_cluster = df_with_cluster.withColumn(\"Row_Id\", F.col(\"id\"))\n",
    "\n",
    "    # Select and order output columns\n",
    "    result = df_with_cluster.select(\n",
    "        \"Row_Id\",\n",
    "        \"fusion_customer_name\",\n",
    "        \"address_line_1\",\n",
    "        \"postal_code\",\n",
    "        \"city\",\n",
    "        \"country\",\n",
    "        \"id\",\n",
    "        \"source_system\",\n",
    "        \"Cluster_Id\",\n",
    "        \"Cluster_Size\"\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6292943626781771,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Dedup_Test_Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
