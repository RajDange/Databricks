{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040e76e5-0f69-400b-9626-982fac5e0cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark Basics and Operations: Beginner Session\n",
    "\n",
    "Welcome to your first PySpark session! In this notebook, we'll learn what PySpark is, how to use it for real-world data analysis, and how it differs from standard Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa3f158b-d573-432a-b2d2-36c616065299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What is PySpark?\n",
    "\n",
    "- **PySpark** is the Python API for Apache Spark.\n",
    "- **Apache Spark** is a powerful open-source engine for big data processing and analytics.\n",
    "- It allows you to process large datasets in parallel across multiple computers (distributed computing).\n",
    "- PySpark is used for data engineering, transformation, machine learning, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a5b6dc-0a88-4242-a7f4-1869e858ae23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How Does PySpark Differ from Python?\n",
    "\n",
    "- **Distributed**: PySpark can process data on many machines at once, while normal Python usually runs on a single computer.\n",
    "- **Lazy Evaluation**: PySpark builds a plan for your data processing and only runs when you need the result.\n",
    "- **DataFrames**: PySpark uses DataFrames for table-like data, similar to pandas but designed for big data.\n",
    "- **Syntax Differences**: Some functions look similar to pandas, but often require using `SparkSession` or `DataFrame` APIs.\n",
    "\n",
    "**Example:**\n",
    "- Python (pandas): `df['age'].mean()`\n",
    "- PySpark: `df.agg({'age': 'mean'}).show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb4d44-aeff-4cbf-8758-377cc591495d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Syntax in PySpark\n",
    "\n",
    "- Variables: Same as Python.\n",
    "- Data Types: int, float, string, etc.\n",
    "- If-Else and Loops: Syntax is the same as Python (but often used less, since you want to use DataFrame operations for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02e6233-cc64-49e0-95f2-6330ded612e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variable declaration\n",
    "a = 10  # integer\n",
    "b = 3.14  # float\n",
    "name = \"Alice\"  # string\n",
    "\n",
    "# If-else\n",
    "if a > 5:\n",
    "    print(\"a is greater than 5\")\n",
    "else:\n",
    "    print(\"a is 5 or less\")\n",
    "\n",
    "# Loop\n",
    "for i in range(3):\n",
    "    print(f\"Loop iteration {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3610f2-2ad6-4736-9f6d-05898415fda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Working with Data in PySpark\n",
    "\n",
    "Usually, you'll use DataFrames for data processing. Let's see how to load data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d632a71-bb64-40b6-833f-652f97782e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data Directly from Databricks Catalog (Database Table)\n",
    "\n",
    "If your data is already in a Databricks database table, you can load it directly using the table API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f058ec3-8868-474c-970f-0b64555c7fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace 'customers' with your actual table name if different\n",
    "customer_table = \"my_database.my_schema.customers\"\n",
    "customer_df = spark.read.table(customer_table)\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1fa723-4621-463f-89a0-0ebf17d416fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DataFrame Operations in PySpark\n",
    "\n",
    "Let's explore some key operations: joins, aggregations, and sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa32be9-dcf2-4e58-9f0b-f5ce2c207442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Joins in PySpark\n",
    "- PySpark supports all major types of joins: inner, left, right, outer.\n",
    "- You typically join two DataFrames on a common column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad545c51-71bc-4f26-80a7-833fe668fe7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create another small DataFrame to demonstrate joins\n",
    "from pyspark.sql import Row\n",
    "\n",
    "city_data = [\n",
    "    Row(location=\"New York\", region=\"East\",time=\"AM\"),\n",
    "    Row(location=\"Los Angeles\", region=\"West\",time=\"AM\"),\n",
    "    Row(location=\"Chicago\", region=\"Midwest\",time=\"AM\"),\n",
    "    Row(location=\"Houston\", region=\"South\",time=\"AM\"),\n",
    "    Row(location=\"Miami\", region=\"South\",time=\"AM\")  # Not present in customer_df\n",
    "]\n",
    "city_df = spark.createDataFrame(city_data)\n",
    "city_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96cbd88-affd-47c8-946d-2418c553d82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Inner Join Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1270bf80-fec5-43d3-8c0a-1fd774938a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inner_join = customer_df.join(city_df, on=\"location\", how=\"inner\")\n",
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742401a8-d5e2-4c93-954d-5b28b956e557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Left Join Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c418ab79-b72b-4f79-8144-c55012ff97ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "left_join = customer_df.join(city_df, on=\"location\", how=\"left\")\n",
    "left_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c76e6e-a993-4939-b048-052986ba7de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Right Join Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fc64b8-a780-45ea-a347-941d4db15f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "right_join = customer_df.join(city_df, on=\"location\", how=\"right\")\n",
    "right_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa821709-3c7e-4da6-b661-34687651a336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Full Outer Join Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a97b58-cd7d-484f-91f6-c2f455ac0621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outer_join = customer_df.join(city_df, on=\"location\", how=\"outer\")\n",
    "outer_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "321f7393-d0fa-424f-a5b4-d5e003bbddff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load this data to output table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca366eab-251c-4d79-81e2-41b135c66cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outer_join.write.mode(\"overwrite\").saveAsTable(\"my_database.my_schema.outer_join_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4595c5cf-6274-433c-bcd5-5daed66cdae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Aggregations in PySpark\n",
    "- You can calculate totals, averages, counts, etc. using `groupBy` and aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8b79f7-23cb-4000-bfe5-d130ffad633e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Total purchase amount by gender\n",
    "new_df = customer_df.groupBy(\"gender\").agg(F.sum(\"purchase_amount\").alias(\"total_purchase\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c0eda7-faba-41ca-bda1-a52a7715ca31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.write.mode(\"overwrite\").saveAsTable(\"my_database.my_schema.new_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a29ff69-364d-4e6c-b1ea-58a5d2de40ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Average purchase amount\n",
    "customer_df.agg(F.avg(\"purchase_amount\").alias(\"avg_purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec681355-f742-480f-9216-583ce5600eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count by location\n",
    "customer_df.groupBy(\"location\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda4e8dc-fa5b-4877-b270-b08d5a5cb63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Minimum and Maximum age\n",
    "customer_df.agg(F.min(\"age\").alias(\"min_age\"), F.max(\"age\").alias(\"max_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37dc4e9a-ceec-4260-b89f-c903e75b0c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Sorting in PySpark\n",
    "- Use `.orderBy()` to sort DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d08942a-ac75-4330-b17a-8cef1dc534e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort by purchase amount descending\n",
    "customer_df.orderBy(F.col(\"purchase_amount\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62630ad3-5a04-4322-9f41-371e7a5c7bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary: Key Points\n",
    "- PySpark lets you process big data efficiently using Python.\n",
    "- It is different from standard Python because it can use multiple machines and is optimized for big data.\n",
    "- Most real work is done via DataFrame APIs instead of Python loops for best performance.\n",
    "- Loading data from Databricks catalog is easy with `spark.read.table()`.\n",
    "- Common operations include joins, aggregations, and sorting, all of which are simple in PySpark.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Basics_and_Operations",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
