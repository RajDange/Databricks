{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5873523a-8d60-4cce-9fc1-fd6b3a3d12be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demonstrating Distributed Computing in PySpark\n",
    "\n",
    "This notebook shows how PySpark performs distributed computing. Each example highlights how Spark splits data and runs operations in parallel across partitions and executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fb9433-8f89-48e6-9a83-eb6e757702d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Parallelizing a Collection\n",
    "\n",
    "Even simple lists can be split and processed in parallel using Spark's RDD API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9fb044-abfe-4ab4-9490-f6d9a8b5958f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create a list of numbers\n",
    "numbers = list(range(1, 100001))\n",
    "\n",
    "# Parallelize into 4 partitions\n",
    "rdd = sc.parallelize(numbers, numSlices=4)\n",
    "\n",
    "# Perform a distributed sum\n",
    "total = rdd.sum()\n",
    "print(\"Sum using distributed computing:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6da3838-100e-4d3f-a54d-a3a5220aaec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Show Partition Distribution\n",
    "\n",
    "You can see how data is split by checking partition info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "353c4a95-dcd6-43d6-95fc-1b042e32f061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def show_partition(index, iterator):\n",
    "    yield f\"Partition: {index}, Count: {sum(1 for _ in iterator)}\"\n",
    "\n",
    "partition_info = rdd.mapPartitionsWithIndex(show_partition).collect()\n",
    "for info in partition_info:\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7c16d5-7b8f-4625-b0bf-f655c63ffc6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. DataFrame Example: Rows per Partition\n",
    "\n",
    "DataFrames are split across partitions too. Let's see the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b11b2c-8044-4319-ae4e-bf29e02f8724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 100000).repartition(4)\n",
    "\n",
    "def partition_counter(iterator):\n",
    "    yield sum(1 for _ in iterator)\n",
    "\n",
    "counts = df.rdd.mapPartitions(partition_counter).collect()\n",
    "for i, c in enumerate(counts):\n",
    "    print(f\"Partition {i}: {c} rows\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    1. 20     2. 20     3. 20    4. 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402730dc-fc70-411d-b07b-bd43ec73b931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Parallel Processing: Heavy Computation Example\n",
    "\n",
    "Let's simulate a slow operation and see how Spark speeds it up by running in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba3466b-1014-420a-8995-28de3173b275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_square(x):\n",
    "    time.sleep(0.01)  # Simulate a heavy computation\n",
    "    return x*x\n",
    "\n",
    "# This will run much faster in parallel than a standard Python loop\n",
    "squared_rdd = rdd.map(slow_square)\n",
    "sample = squared_rdd.take(10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53b4087-343d-4a47-8057-eebcea13f2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Explaining the Distributed Nature\n",
    "\n",
    "- Spark breaks up your job into tasks and runs them in parallel across executors/cores.\n",
    "- You can control the number of partitions for parallelism.\n",
    "- Even on a laptop, Spark will use all available CPU cores; on a cluster, it will use multiple machines.\n",
    "- Check the Spark UI to see stages and tasks distributed."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Distributed_Computing_Demonstration_Version2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
